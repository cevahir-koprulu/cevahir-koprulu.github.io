<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Cevahir Koprulu's Personal Website</title>
  
  <meta name="author" content="Cevahir Koprulu">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
	<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üåê</text></svg>">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Cevahir Koprulu</name>
              </p>
              <p>I am a Ph.D. student in <a href="https://www.ece.utexas.edu/"> Department of Electrical and Computer Engineering</a>, <a href="https://www.utexas.edu/">University of Texas at Austin</a>. 
              I am advised by Prof. <a href="https://www.ae.utexas.edu/people/faculty/faculty-directory/topcu">Ufuk Topcu</a>.
              I received my B.Sc degree from the Department of Electrical Engineering at Bilkent University, 
              where I worked with Assoc. Prof. <a href="https://scholar.google.com/citations?user=UPyjGxgAAAAJ&hl=en">Yildiray Yildiz</a>. </p>

              <p style="text-align:center">
                <a href="mailto:cevahir.koprulu@utexas.edu">Email</a> &nbsp/&nbsp
                <a href="data/CV.pdf">CV</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=3vnXbccAAAAJ&hl=en&authuser=1">Google Scholar</a> &nbsp/&nbsp
                <a href="https://github.com/cevahir-koprulu">Github</a> &nbsp/&nbsp
                <a href="https://www.linkedin.com/in/cevahir-koprulu/">LinkedIn</a>
              </p>

            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/pp.jpg"><img style="width:100%;max-width:100%" alt="profile photo" src="images/pp.jpg" class="hoverZoomLink"></a>
              <figcaption></figcaption>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:10px;width:100%;vertical-align:middle">
              <h1>Research</h1>
              <p>
              My research interests are reinforcement learning and control theory. 
              In particular, my projects focus on curriculum learning, multi-task learning and model-based reinforcement learning. 

              <h2>Curriculum Learning for Reinforcement Learning</h2> 
              The design of task sequences, i.e., curricula, improves the performance of reinforcement learning (RL) agents and speeds up the convergence in complex tasks.
              An effective curriculum typically begins with easy tasks and gradually changes them toward the target tasks.
              Common approaches require manually tailoring the curricula to identify easy and hard tasks, which requires domain knowledge that might be unavailable.
              My research focus on developing automated curriculum generations algorithms for long-horizon planning tasks and settings with rare and risky tasks.
              <p></p>
              <h3>Publications</h3>

              <a href="">
                <papertitle>Risk-aware Curriculum Generation for Heavy-tailed Task Distributions </papertitle>
              </a>
              <br> 
							<strong>Cevahir Koprulu</strong>, 
              Thiago D. Sim√£o, 
              Nils Jansen,
              Ufuk Topcu
              <br>
              <em> Conference on Uncertainty in Artifical Intelligence (UAI)</em>, 2023
              <br>
              <p></p>


              <a href="">
              <papertitle>Reward-Machine-Guided, Self-Paced Reinforcement Learning </papertitle>
              </a>
              <br> 
							<strong>Cevahir Koprulu</strong>, 
              Ufuk Topcu
              <br>
              <em> Conference on Uncertainty in Artifical Intelligence (UAI)</em>, 2023
              <br>
              <p></p>

              <h2>Decentralized Data Sharing</h2> 
              Federated learning (FL) is commonly used to efficiently train deep neuron networks (DNN) among a group of devices. However, one major restriction of FL is that it only applies to homogeneous neuron networks, i.e., networks with identical structure and layers. Thus, my projects focus on how to share raw data, but not gradients, among devices to retrain neuron networks, and most importantly, to avoid the previous restriction. Moreover, the projects discuss under network bandwidth constraints, how to share a limited number of valuable data without violating privacy of devices. Topics related to these projects are: out-of-distribution (OoD) detectors, data valuation, active learning, data sanitizing, differential privacy, and distributed optimization. 

              <h3>Publications</h3>
              
              <a href="">
                <papertitle>Reward-Machine-Guided, Self-Paced Reinforcement Learning </papertitle>
              </a>
              <br> 
							<strong>Cevahir Koprulu</strong>, 
              Ufuk Topcu
              <br>
              <em> International Conference on Autonomous Agents and Multiagent Systems
                (AAMAS)</em>, 2023 (accepted as Extended Abstract)
              <br>
              <p></p>

              <h2>Learning Reward Machines and Policies</h2> 
              We study the problem of reinforcement learning for a task encoded by a reward machine. 
              The task is defined over a set of properties in the environment, called atomic propositions, and represented by Boolean variables. 
              One unrealistic assumption commonly used in the literature is that the truth values of these propositions are accurately known. 
              In real situations, however, these truth values are uncertain since they come from sensors that suffer from imperfections. 
              At the same time, reward machines can be difficult to model explicitly, especially when they encode complicated tasks. 
              We develop a reinforcement-learning algorithm that infers a reward machine that encodes the underlying task while learning how to execute it, despite the uncertainties of the propositions' truth values.              <p></p>
              <h3>Publications</h3>

              <a href="https://cverginis.github.io/publications/journals/AI_22.pdf">
                <papertitle>Joint Learning of Reward Machines and Policies in Environments with Partially Known Semantics
                </papertitle>
              </a>
              <br>
              Christos Verginis, 
							<strong>Cevahir Koprulu</strong>, 
              Sandeep Chinchali, 
              Ufuk Topcu
              <br>
              <em>Under review</em>
              <br>
              <p></p>
              
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;font-size:small;">
                This website is based on Jon Barron's <a href="https://github.com/jonbarron/jonbarron_website">source code</a>.
                His website can be found <a href="https://jonbarron.info/">here</a>.
                <br>
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
</body>

</html>
