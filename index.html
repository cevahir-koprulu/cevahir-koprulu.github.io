<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Cevahir Koprulu's Personal Website</title>
  
  <meta name="author" content="Cevahir Koprulu">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
	<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üåê</text></svg>">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Cevahir Koprulu</name>
              </p>
              <p>I am a Ph.D. student at <a href="https://www.ece.utexas.edu/">the Department of Electrical and Computer Engineering</a>, <a href="https://www.utexas.edu/">University of Texas at Austin</a>. 
              I am advised by Prof. <a href="https://scholar.google.com/citations?user=jeNGFfQAAAAJ&hl=en">Ufuk Topcu</a>.
              I received my B.Sc degree from <a href="https://ee.bilkent.edu.tr/en/">the Department of Electrical and Electronics Engineering</a> 
              at <a href="https://w3.bilkent.edu.tr/bilkent/">Bilkent University</a>, 
              where I worked with Assoc. Prof. <a href="https://scholar.google.com/citations?user=UPyjGxgAAAAJ&hl=en">Yildiray Yildiz</a>. </p>

              <p style="text-align:center">
                <a href="mailto:cevahir.koprulu@utexas.edu">Email</a> &nbsp/&nbsp
                <a href="data/CV.pdf">CV</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=3vnXbccAAAAJ&hl=en&authuser=1">Google Scholar</a> &nbsp/&nbsp
                <a href="https://github.com/cevahir-koprulu">Github</a> &nbsp/&nbsp
                <a href="https://www.linkedin.com/in/cevahir-koprulu/">LinkedIn</a>
                <a href="https://twitter.com/cevahirkoprulu/">Twitter</a> &nbsp/&nbsp
                <a href="https://autonomy.oden.utexas.edu/">Center for Autonomy</a>
              </p>

            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/Cevahir Koprulu  2.JPG"><img style="width:100%;max-width:100%" alt="profile photo" src="images/Cevahir Koprulu  2.JPG" class="hoverZoomLink"></a>
              <figcaption>Taken at UT Austin, 2024</figcaption>
            </td>
          </tr>
        </tbody></table>

        <tr>
          <td style="padding:10px;width:100%;vertical-align:middle">
            <h1>News</h1> 
            <ul>
              <li><span> Aug 2024: I finished my summer research internship at Honda Research Institute! </li>
              <li><span> Aug 2023: New <a href="https://www.sciencedirect.com/science/article/pii/S0004370224000821"> journal paper </a> at Artificial Intelligence 2024!</li>
              <li><span> Aug 2024: Workshops! Gave a talk at RLBrew and presented the same <a href="https://openreview.net/forum?id=EaaWjhIQwr7"> paper </a> at RLSW during RLC 2024! </li>
              <li><span> Aug 2023: Two papers presented at UAI 2023!: <a href="https://proceedings.mlr.press/v216/koprulu23b.html"> RACGEN </a> and <a href="https://proceedings.mlr.press/v216/koprulu23a.html"> RM-Guided SPRL </a> </li>
              <li><span> June 2023: New extended abstract at AAMAS 2023: <a href="https://dl.acm.org/doi/pdf/10.5555/3545946.3598964"> RM-Guided SPRL </a> </li>
              <li><span> Aug 2021: New <a href="https://ieeexplore.ieee.org/abstract/document/9658815"> paper </a> at CCTA 2021!</li>
            </ul>
          </td>
          </tr>

          https://openreview.net/forum?id=EaaWjhIQwr

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:10px;width:100%;vertical-align:middle">
              <h1>Research</h1>
              <p>
              I do research on reinforcement learning, with a focus on multi-task RL, curriculum learning, and meta RL. 

              <h2>Curriculum Learning for Reinforcement Learning</h2> 
              The design of task sequences, i.e., curricula, improves the performance of reinforcement learning (RL) agents and speeds up the convergence in complex tasks.
              An effective curriculum typically begins with easy tasks and gradually changes them toward the target tasks.
              Common approaches require manually tailoring the curricula to identify easy and hard tasks, which requires domain knowledge that might be unavailable.
              My research focus on developing automated curriculum generations algorithms for long-horizon planning tasks and settings with rare and risky tasks.
              <p></p>
              <h3>Publications</h3>

              <a href="https://proceedings.mlr.press/v216/koprulu23b.html">
                <papertitle>Risk-aware Curriculum Generation for Heavy-tailed Task Distributions </papertitle>
              </a>
              <br> 
							<strong>Cevahir Koprulu</strong>, 
              Thiago D. Sim√£o, 
              Nils Jansen,
              Ufuk Topcu
              <br>
              <em> Conference on Uncertainty in Artifical Intelligence (UAI)</em>, 2023
              <br>
              <p></p>

              <a href="https://proceedings.mlr.press/v216/koprulu23a.html">
              <papertitle>Reward-Machine-Guided, Self-Paced Reinforcement Learning </papertitle>
              </a>
              <br> 
							<strong>Cevahir Koprulu</strong>, 
              Ufuk Topcu
              <br>
              <em> Conference on Uncertainty in Artifical Intelligence (UAI)</em>, 2023
              <br>
              <p></p>

              <a href="https://dl.acm.org/doi/abs/10.5555/3545946.3598964">
                <papertitle>Reward-Machine-Guided, Self-Paced Reinforcement Learning </papertitle>
              </a>
              <br> 
							<strong>Cevahir Koprulu</strong>, 
              Ufuk Topcu
              <br>
              <em> International Conference on Autonomous Agents and Multiagent Systems
                (AAMAS)</em>, 2023 (accepted as Extended Abstract)
              <br>
              <p></p>

              <h2>Learning Reward Machines and Policies</h2> 
              We study the problem of reinforcement learning for a task encoded by a reward machine. 
              The task is defined over a set of properties in the environment, called atomic propositions, and represented by Boolean variables. 
              One unrealistic assumption commonly used in the literature is that the truth values of these propositions are accurately known. 
              In real situations, however, these truth values are uncertain since they come from sensors that suffer from imperfections. 
              At the same time, reward machines can be difficult to model explicitly, especially when they encode complicated tasks. 
              We develop a reinforcement-learning algorithm that infers a reward machine that encodes the underlying task while learning how to execute it, despite the uncertainties of the propositions' truth values.              <p></p>
              
              <h3>Publications</h3>

              <a href="https://cverginis.github.io/publications/journals/AI_22.pdf">
                <papertitle>Joint Learning of Reward Machines and Policies in Environments with Partially Known Semantics
                </papertitle>
              </a>
              <br>
              Christos Verginis, 
							<strong>Cevahir Koprulu</strong>, 
              Sandeep Chinchali, 
              Ufuk Topcu
              <br>
              <em>Under review</em>
              <br>
              <p></p>
              
              <h2>Level-k Game Theory to Model Human Drivers</h2>
              Level-k game theory is a hierarchical multi-agent decision-making model where a level-k player is a best responder to a level-(k-1) player.
              In this work, we studied level-k game theory to model reasoning levels of human drivers. 
              Different from existing methods, we proposed a dynamic approach, 
              where the actions are the levels themselves, resulting in a dynamic behavior. 
              The agent adapts to its environment by exploiting different behavior models as available moves to choose from, depending on the requirements of the traffic situation. 

              <h3>Publications</h3>

              <a href="https://ieeexplore.ieee.org/abstract/document/9658815">
                <papertitle>Act to Reason: A Dynamic Game Theoretical Driving Model for Highway Merging Applications
                </papertitle>
              </a>
              <br>
							<strong>Cevahir Koprulu</strong>, 
              Yildiray Yildiz
              <br>
              <em> IEEE Conference on Control Technology and Applications (CCTA)</em>, 2021
              <br>
              <p></p>

              <a href="https://arxiv.org/abs/2101.05399">
                <papertitle>Act to reason: A dynamic game theoretical model of driving

              </a>
              <br>
							<strong>Cevahir Koprulu</strong>, 
              Yildiray Yildiz
              <br>
              <em>ArXiV</em>, 2021
              <br>
              <p></p>

        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;font-size:small;">
                This website is based on Jon Barron's <a href="https://github.com/jonbarron/jonbarron_website" style="font-size:small;"> source code</a>.
                His website can be found <a href="https://jonbarron.info/" style="font-size:small;">here</a>.
                <br>
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
</body>

</html>
