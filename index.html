<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Cevahir Koprulu's Personal Website</title>
  
  <meta name="author" content="Cevahir Koprulu">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
	<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üåê</text></svg>">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Cevahir Koprulu</name>
              </p>
              <p>I am a Ph.D. student at <a href="https://www.ece.utexas.edu/">the Department of Electrical and Computer Engineering</a>, <a href="https://www.utexas.edu/">University of Texas at Austin</a>. 
              I am advised by Prof. <a href="https://scholar.google.com/citations?user=jeNGFfQAAAAJ&hl=en">Ufuk Topcu</a>.
              I received my B.Sc degree from <a href="https://ee.bilkent.edu.tr/en/">the Department of Electrical and Electronics Engineering</a> 
              at <a href="https://w3.bilkent.edu.tr/bilkent/">Bilkent University</a>, 
              where I worked with Assoc. Prof. <a href="https://scholar.google.com/citations?user=UPyjGxgAAAAJ&hl=en">Yildiray Yildiz</a>. </p>

              <p style="text-align:center">
                <a href="mailto:cevahir.koprulu@utexas.edu">Email</a> &nbsp/&nbsp
                <a href="data/CV.pdf">CV</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=3vnXbccAAAAJ&hl=en&authuser=1">Google Scholar</a> &nbsp/&nbsp
                <a href="https://github.com/cevahir-koprulu">Github</a> &nbsp/&nbsp
                <a href="https://www.linkedin.com/in/cevahir-koprulu/">LinkedIn</a>
                <a href="https://twitter.com/cevahirkoprulu/">Twitter</a> &nbsp/&nbsp
                <a href="https://autonomy.oden.utexas.edu/">Center for Autonomy</a>
              </p>

            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/Cevahir Koprulu  2.JPG"><img style="width:100%;max-width:100%" alt="profile photo" src="images/Cevahir Koprulu  2.JPG" class="hoverZoomLink"></a>
              <figcaption>  Taken at UT Austin, 2024.</figcaption>
            </td>
          </tr>
        </tbody></table>

        <tr>
          <td style="padding:10px;width:100%;vertical-align:middle">
            <h1>News</h1> 
            <ul>
              <li><span> Jan 2025: Two papers got accepted by ICLR 2025! <a href="https://openreview.net/forum?id=f3QR9TEERH&referrer=%5Bthe%20profile%20of%20Cevahir%20Koprulu%5D(%2Fprofile%3Fid%3D~Cevahir_Koprulu1)">Safety-Prioritizing Curricula</a> and <a href="https://openreview.net/forum?id=hxUMQ4fic3&referrer=%5Bthe%20profile%20of%20Cevahir%20Koprulu%5D(%2Fprofile%3Fid%3D~Cevahir_Koprulu1)">Neural SDEs for Offline RL</a></li>
              <li><span> Aug 2024: I finished my summer research internship at Honda Research Institute! </li>
              <li><span> Aug 2023: New <a href="https://www.sciencedirect.com/science/article/pii/S0004370224000821"> journal paper </a> at Artificial Intelligence 2024!</li>
              <li><span> Aug 2024: Workshops! Gave a talk at RLBRew and presented the same <a href="https://openreview.net/forum?id=EaaWjhIQwr7"> paper </a> at RLSW during RLC 2024! </li>
              <li><span> Aug 2023: Two papers presented at UAI 2023!: <a href="https://proceedings.mlr.press/v216/koprulu23b.html"> RACGEN </a> and <a href="https://proceedings.mlr.press/v216/koprulu23a.html"> RM-Guided SPRL </a> </li>
              <li><span> June 2023: New <a href="https://dl.acm.org/doi/pdf/10.5555/3545946.3598964"> extended abstract </a> at AAMAS 2023! </li>
              <li><span> Aug 2021: New <a href="https://ieeexplore.ieee.org/abstract/document/9658815"> paper </a> at CCTA 2021!</li>
            </ul>
          </td>
        </tr>

        <tr>
          <td style="padding:16px;width:20%;vertical-align:middle">
            <img src="images/scg.png" alt="clean-usnob" width="160" height="160">
          </td>
          <td style="padding:8px;width:80%;vertical-align:middle">
            <a href="projects/scg.html">
          <span class="papertitle">Safety-Prioritizing Curricula for Constrained Reinforcement Learning
    </span>
            </a>
            <br>
            <strong>Cevahir Koprulu</strong>,
            <a href="https://scholar.google.com/citations?user=jeNGFfQAAAAJ&hl=en">Thiago D. Sim√£o</a>
            <a href="https://scholar.google.com/citations?user=jeNGFfQAAAAJ&hl=en">Nils Jansen</a>
            <a href="https://scholar.google.com/citations?user=jeNGFfQAAAAJ&hl=en">Ufuk Topcu</a>
            <br>
            <em>ICLR</em>, 2025 &nbsp</font>
            <br>
            <a href="projects/scg.html">Project Page</a>
            /
            <a href="https://openreview.net/forum?id=f3QR9TEERH&referrer=%5Bthe%20profile%20of%20Cevahir%20Koprulu%5D(%2Fprofile%3Fid%3D~Cevahir_Koprulu1)">OpenReview</a>
            <p></p>
            <p>
              We propose a safe curriculum generation method that reduces safety constraint violations during training while boosting the learning speed of constrained RL agents.
            </p>
          </td>


        </tr>

            <tr>
            <td style="padding:10px;width:100%;vertical-align:middle">
              <h1>Research</h1>
              <p>
              I do research on reinforcement learning (RL), with a focus on generalization and adaptation. 
              My publications can be found on <a href="https://scholar.google.com/citations?user=3vnXbccAAAAJ&hl=en&authuser=1">my Google Scholar page</a>, and some are listed with details below. 
              During my research internship at HRI, I developed an action advising framework, Gen2Spec, that distills knowledge from generalist meta RL agents to specialists in a continual learning setting. 
              Recently, I have been focusing on the following research directions:
              <ul>
                <li><span><strong>CL for Multi-Modality RL:</strong> We study automated curriculum generation for domains with multiple modalities.</li>
                <li><span><strong>CL for Outliers in Fine-tuning:</strong> We investigate how to address outlier tasks/scenarios in fine-tuning of foundation models.</li>
              </ul>

              <!-- More recently, I have been working on leveraging learned task representations to develop adaptation methods for meta RL. 
              In addition, I have been investigating how to use such representations of primitive skills to teach RL agents their more complex combinations under a curriculum learning framework. -->
              
              <h2>Curriculum Learning for Reinforcement Learning</h2> 
              The design of task sequences, i.e., curricula, improves the performance of RL agents and speeds up the convergence in complex tasks.
              An effective curriculum typically begins with easy tasks and gradually changes them toward the target tasks.
              Common approaches require manually tailoring the curricula to identify easy and hard tasks, which requires domain knowledge that might be unavailable.
              My research focuses on developing automated curriculum generations algorithms that introduce a notion of risk, address constrained RL, and exploits task specifications.
              <p></p>
              <h3>Publications</h3>

              <a href="https://openreview.net/forum?id=f3QR9TEERH&referrer=%5Bthe%20profile%20of%20Cevahir%20Koprulu%5D(%2Fprofile%3Fid%3D~Cevahir_Koprulu1)">
                <papertitle>Safety-Prioritizing Curricula for Constrained Reinforcement Learning</papertitle>
              </a>
              <br> 
							<strong>Cevahir Koprulu</strong>, 
              Thiago D. Sim√£o, 
              Nils Jansen,
              Ufuk Topcu
              <br>
              <em> International Conference on Learning Representations (ICLR)</em>, 2025
              <br>
              <p></p>
 
              <a href="https://proceedings.mlr.press/v216/koprulu23b.html">
                <papertitle>Risk-aware Curriculum Generation for Heavy-tailed Task Distributions </papertitle>
              </a>
              <br> 
							<strong>Cevahir Koprulu</strong>, 
              Thiago D. Sim√£o, 
              Nils Jansen,
              Ufuk Topcu
              <br>
              <em> Conference on Uncertainty in Artifical Intelligence (UAI)</em>, 2023
              <br>
              <p></p>

              <a href="https://proceedings.mlr.press/v216/koprulu23a.html">
              <papertitle>Reward-Machine-Guided, Self-Paced Reinforcement Learning </papertitle>
              </a>
              <br> 
							<strong>Cevahir Koprulu</strong>, 
              Ufuk Topcu
              <br>
              <em> Conference on Uncertainty in Artifical Intelligence (UAI)</em>, 2023
              <br>
              <p></p>

              <a href="https://dl.acm.org/doi/abs/10.5555/3545946.3598964">
                <papertitle>Reward-Machine-Guided, Self-Paced Reinforcement Learning </papertitle>
              </a>
              <br> 
							<strong>Cevahir Koprulu</strong>, 
              Ufuk Topcu
              <br>
              <em> International Conference on Autonomous Agents and Multiagent Systems
                (AAMAS)</em>, 2023 (accepted as Extended Abstract)
              <br>
              <p></p>
              <h3>Workshops</h3>
              <a href="https://proceedings.mlr.press/v216/koprulu23b.html">
                <papertitle>Prioritizing Safety via Curriculum Learning </papertitle>
              </a>
              <br> 
							<strong>Cevahir Koprulu</strong>, 
              Thiago D. Sim√£o, 
              Nils Jansen,
              Ufuk Topcu
              <br>
              <em> RLBRew and RLSW at Reinforcement Learning Conference </em>, 2024
              <br>
              <p></p>

              <h2>Learning Reward Machines and Policies</h2> 
              We study the problem of reinforcement learning for a task encoded by a reward machine. 
              The task is defined over a set of properties in the environment, called atomic propositions, and represented by Boolean variables. 
              One unrealistic assumption commonly used in the literature is that the truth values of these propositions are accurately known. 
              In real situations, however, these truth values are uncertain since they come from sensors that suffer from imperfections. 
              At the same time, reward machines can be difficult to model explicitly, especially when they encode complicated tasks. 
              We develop a reinforcement-learning algorithm that infers a reward machine that encodes the underlying task while learning how to execute it, despite the uncertainties of the propositions' truth values.              <p></p>
              
              <h3>Publications</h3>

              <a href="https://www.sciencedirect.com/science/article/pii/S0004370224000821">
                <papertitle>Joint Learning of Reward Machines and Policies in Environments with Partially Known Semantics
                </papertitle>
              </a>
              <br>
              Christos Verginis, 
							<strong>Cevahir Koprulu</strong>, 
              Sandeep Chinchali, 
              Ufuk Topcu
              <br>
              <em>Aritificial Intelligence, 2024</em>
              <br>
              <p></p>
              
              <h2>Level-k Game Theory to Model Human Drivers</h2>
              Level-k game theory is a hierarchical multi-agent decision-making model where a level-k player is a best responder to a level-(k-1) player.
              In this work, we studied level-k game theory to model reasoning levels of human drivers. 
              Different from existing methods, we proposed a dynamic approach, 
              where the actions are the levels themselves, resulting in a dynamic behavior. 
              The agent adapts to its environment by exploiting different behavior models as available moves to choose from, depending on the requirements of the traffic situation. 

              <h3>Publications</h3>

              <a href="https://ieeexplore.ieee.org/abstract/document/9658815">
                <papertitle>Act to Reason: A Dynamic Game Theoretical Driving Model for Highway Merging Applications
                </papertitle>
              </a>
              <br>
							<strong>Cevahir Koprulu</strong>, 
              Yildiray Yildiz
              <br>
              <em> IEEE Conference on Control Technology and Applications (CCTA)</em>, 2021
              <br>
              <p></p>

              <a href="https://arxiv.org/abs/2101.05399">
                <papertitle>Act to reason: A dynamic game theoretical model of driving

              </a>
              <br>
							<strong>Cevahir Koprulu</strong>, 
              Yildiray Yildiz
              <br>
              <em>ArXiV</em>, 2021
              <br>
              <p></p>

        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;font-size:small;">
                This website is based on Jon Barron's <a href="https://github.com/jonbarron/jonbarron_website" style="font-size:small;"> source code</a>.
                His website can be found <a href="https://jonbarron.info/" style="font-size:small;">here</a>.
                <br>
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
</body>

</html>
